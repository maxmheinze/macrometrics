---
title: '**Advanced Macroeconometrics -- Assignment 2**'
author:
  - "Max Heinze (h11742049@s.wu.ac.at)"
  - "Gabriel Konecny (h11775903@s.wu.ac.at)"
  - "Patrick Sch√ºssele (h11712262@s.wu.ac.at)"
date: "May 10, 2023"
output: 
  pdf_document:
    toc: true
    includes:
      in_header: !expr file.path(rprojroot::find_rstudio_root_file(), "helper", "wrap_code.tex")
header-includes: 
   - \usepackage{tcolorbox}
   - \usepackage{bm}
papersize: a4
geometry: margin = 2cm
urlcolor: Mahogany
---

```{r, setup, include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
```

\vspace{2em}

\begin{tcolorbox}
\centering \itshape The executable code that was used in compiling the assignment is available on GitHub at \url{https://github.com/maxmheinze/macrometrics}.
\end{tcolorbox}

\newpage

# Exercise 1
## Exercise 1.1 - Convergence of Means of $n$ Normal Draws

We draw 100 times from a $\mathcal{N}(5,9)$ distribution and plot the cumulative mean as well as the expected value.

```{r, echo = FALSE, out.width="70%", fig.align="center"}
set.seed(2345)

n <- 100
normal_data <- data.frame(n=1:n, x=rnorm(n, mean=5, sd=9), mean=NA)

for (i in 1:n) {
  normal_data$mean[i] <- mean(normal_data$x[1:i])
}

plot(x=normal_data$n, y=normal_data$mean, type="l", lwd=2,
     main="Convergence of Means: Normal Distribution", xlab="n", ylab="")
abline(h=5, col="red", lwd=2)
legend(50,-2, c("Cumulative mean", "True mean"),
       lwd=c(2,2), col=c("black", "red"))
```

We can easily see that the cumulative mean, that is,

\[
  \frac{1}{n}\sum^n_{i=1}x_i, \quad n = 1,\dots,100,
\]

converges to the true mean, the expected value of the distribution, $\mathrm{E}(N(5,9))=5$.


## Exercise 1.2 - Convergence of Means of $n$ Draws From a Cauchy Distribution

Next, we draw 10,000 times from a Cauchy distribution as specified and plot the cumulative mean.

```{r, echo = FALSE, out.width="70%", fig.align="center"}
set.seed(2345)

n <- 10000

cauchy_data <- data.frame(n=1:n, x=rnorm(n, mean=0, sd=1)/rnorm(n, mean=0, sd=1), mean=NA)

for (i in 1:n) {
  cauchy_data$mean[i] <- mean(cauchy_data$x[1:i])
}

plot(x=cauchy_data$n, y=cauchy_data$mean, type="l", lwd=2,
     main="Convergence of Means: Cauchy Distribution", xlab="N draws", ylab="")

legend(6000, 8, "Cumulative mean", lwd=2, col="black")
```

This time, we do not observe convergence of the cumulative mean. Rather, we observe repeated random jumps in the cumulative mean. This is due to the Cauchy distribution not having an expected value that it could converge to.

\newpage

# Exercise 2
## Exercise 2.1 - The conjugate prior

A Covid test can be represented by a Bernoulli trial, where our random variable takes over value = 1 if test is *positive* and value = 0 if *negative*. As we conduct the tests for 20 colleagues, we have a sequence of Bernoulli trials, where the random variable now is the sum of all single Bernoulli trials. This random variable now follows a Binomial distribution. That is, our likelihood follows a Binomial distribution.\footnote{Bernoulli distribution is just a special case of a Binomial distribution with n=1.} 

The class of conjugate priors for a Binomial distribution are **Beta($\alpha$, $\beta$)**  distributions where $\alpha$, $\beta$ $\in$ $\mathbb{R}_+$.

Notice that we have vector of observations $y = (y_1,...,y_n)$ for n days, where $y_i$ contains 20 observations, one for each colleague. Thus $y_i \sim Binom(20,\theta)$ Since the n observations across time are iid the likelihood originates from $Binom(20n,\theta)$

Thus the distribution of $y_i$, the likelihood function and our prior are respectively:

$$
\begin{aligned}
p(y_i\mid\theta) = {20 \choose k_i} \theta^{k_i} (1-\theta)^{20-{k_i}}  \\
p(y\mid\theta) = {N \choose S_n} \theta^{S_n} (1-\theta)^{N-{S_n}} \propto \theta^{S_n} (1-\theta)^{N-S_n}  \\
p(\theta) = p(\theta\mid\alpha,\beta) = \frac{1}{B(\alpha,\beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1} \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}
\end{aligned}
$$
where $k_i = \sum_j^{20}y_{ij}$, $S_n = \sum_i^n\sum_j^{20}y_{ij}$ and $N = 20n$. We could simplify the sum and rewrite is as: $S_n = \sum_i^{N}y_{i}$ where $y_i$ now denotes individual test results for all days.

Then the posterior can be derived using Bayes formula:  

$$
\begin{aligned}
p(\theta|y) &\propto p(y|\theta) \cdot p(\theta)\\
&\propto \theta^{S_n} \cdot (1-\theta)^{N-S_n} \cdot  \theta^{\alpha-1} \cdot (1-\theta)^{\beta-1} \\ 
&= \theta^{\alpha + S_n - 1} \cdot (1-\theta)^{\beta + N- S_n - 1} \\ 
\implies p(\theta|y) &\sim Beta(\alpha + S_n, \beta + N- S_n)
\end{aligned}
$$
Our posterior is proportional to a Beta distribution. Thus the Beta prior is indeed a conjugate prior.


## Exercise 2.2 - Point estimates of $\theta$

30 observations for each of the 20 individuals are in total 600 observations. In total, 10 of the 600 observations/tests are positive. Then, given the fact that our prior as well as our posterior follows a beta distribution, we can look up the formulae of point estimators such as *mean*, *median*, *mode*, etc. for a beta distribution $Beta(\alpha, \beta)$. Applied to the case at hand, we obtain for the prior the following point estimators:

- Mean: $E[\theta] = \frac{\alpha_0}{\alpha_0+\beta_0}$
- Median
  - for $\alpha_0,\beta_0>1$: $\frac{\alpha_0-\frac{1}{3}}{\alpha_0+\beta_0-\frac{2}{3}}$
  - for $\alpha,\beta=1$: Median = Mean. 
- Mode:
  - for $\alpha_0,\beta_0>1$: $\frac{\alpha_0-1}{\alpha_0+\beta_0-2}$
  - for $\alpha_0,\beta_0=1$: Any value $x\in(0,1)$ is mode.
  
Note, that for posterior point estimators, we simply replace $\alpha_0$ by $\alpha_1 = \alpha_0+S_n$, and $\beta_0$ by $\beta_1 = \beta_0+N-S_n$, where $S_n = 10$ is the number of successes; i.e. positive test results, and $N=600$ is the number of observations.

To see how these point estimators change with different values for $\alpha_0$, and $\beta_0$, see the table underneath. This gives a first indication of how different beta priors affect the posterior.

```{r, include=TRUE,echo=FALSE, message=FALSE,warning=FALSE}
m <- function(a,b){
  round((a)/(a+b), 4)
  }

med <- function(a,b){
  round((a-1/3)/(a+b-2/3),4)
  }

mod <- function(a,b){
  ifelse(a == 1 & b == 1, 
          "any in (0,1)", 
          round((a-1)/(a+b-2),4))
  }

a_1 = function(a, s_n = 10){
  a+s_n
  }

b_1 = function(b, s_n = 10, n = 600) {
  b+n-s_n
  }

a_0 = c(1,2,10,0.5,1,2,10)
b_0 = c(1,10,2,0.5,2,1,250)

df = as.data.frame(cbind(a_0, b_0))

library(dplyr) 
# as we make use of the summarise() command from the dplyr packacge

tbl = as.matrix(dplyr::summarise(.data = df, a_0 = df$a_0
            , b_0 = df$b_0
            , prior_mean = m(a_0, b_0)
            , prior_med = med(a_0, b_0)
            , prior_mod = mod(a_0, b_0)
            , a_1 = a_1(a_0)
            , b_1 = b_1(b_0)
            , post_mean = m(a_1, b_1)
            , post_med = med(a_1, b_1)
            , post_mod = mod(a_1, b_1)))

row.names(tbl) = rep("", nrow(tbl))
tbl = as.table(tbl)

knitr::kable(tbl, caption = "Point estimators for various alpha and beta.")

```

## Exercise 2.3 - Prior information and different priors

To visualise the impact different beta priors have on the posterior distribution, see the corresponding density plots that make use of selected $\alpha_0$ and $\beta_0$ shape parameters used in the table above. 

```{r, echo = FALSE}
#define range
p = seq(0,1, length=100)

#plot several Beta distributions
plot(p, dbeta(p, a_0[1], b_0[1]), ylab='density', type ='l', col='black', main="Priors",ylim=c(0,10))


lines(p, dbeta(p, a_0[2], b_0[2]), col='red') 
lines(p, dbeta(p, a_0[3], b_0[3]), col='blue')
lines(p, dbeta(p, a_0[4], b_0[4]), col='green')
lines(p, dbeta(p, a_0[7], b_0[7]), col='purple')

#add legend
legend("topright", c('Beta(1, 1)','Beta(2, 10)','Beta(10,2)', 'Beta(0.5,0.5)'),
       lty=c(1,1,1),col=c('black', 'red', 'blue', 'green'))


```


```{r, echo = FALSE}
#plot several Beta distributions
plot(p, dbeta(p, a_1(a_0)[1], b_1(b_0)[1]), ylab='density', type ='l', col='black', main="Posteriors",ylim=c(0,80))


lines(p, dbeta(p, a_1(a_0)[2], b_1(b_0)[2]), col='red') 
lines(p, dbeta(p, a_1(a_0)[3], b_1(b_0)[3]), col='blue')
lines(p, dbeta(p, a_1(a_0)[4], b_1(b_0)[4]), col='green')
lines(p, dbeta(p, a_1(a_0)[7], b_1(b_0)[7]), col='purple')

#add legend
legend("topright", c('Beta(1, 1)','Beta(2, 10)','Beta(10,2)', 'Beta(0.5,0.5)'),
       lty=c(1,1,1),col=c('black', 'red', 'blue', 'green'))

```

For $\alpha_0 = \beta_0 = 1$ the prior is simply a uniform(1,1) distribution. This uninformative prior gives us a posterior distribution which is narrowly centered around approx. 0.02. As we see in the graphs above, a change in the prior parameters changes the pattern for the posterior only slightly in absolute terms. For arbitrary values $\alpha_0 = \beta_0>1$ prior has the property that mean = median = mode\footnote{for $\alpha_0 = \beta_0=1$, mean = meadian = 0.5, and mode is any value in interval (0,1).}, but this does not hold for the posterior as $\alpha_1$ and $\beta_1$  are non-proportional linear combinations of $\alpha_0$ and $\beta_0$.   

For an increase in $\alpha_0$ holding $\beta_0$ constant we see an increase in posterior point estimates. If we increase $\beta_0$ holding $\alpha_0$ constant, estimates decrease.

As for sources of prior information, information for this particular problem can stem from previous research or available data. For one of our Beta-priors we tried to match the mean to the most recent value of of COVID-19 test rates, which seem to be around 3.7%. After trying out few different parameters, this led us to use Beta(10,250), which has mean close to 3.7%. For more serious work, one would solve the equation for mean, but also try to match at least the variance of prior to the data as well.


## Exercise 2.4 - Improving the model
First, the assumption of *identical distribution* might not be fulfilled in real life, because the situation outside of our trial can influence the probability of testing positive on a particular day. Ceteris paribus, on a day with high Covid incidence, the probability that one of our employees will get in contact with an infected person and contracts Covid is higher. Also the life-style of the employees can influence their probability of testing positive. The people who tend to participate more in social activities might be more likely to test positive as compared to people who mostly isolate at home. Also someone who already had covid might be partially immune thus effecting the probability.
Second, if one of our employees in the office tests positive on a particular day, then this employee will probably test positive on the second day as well. The probability that other colleagues test positive on the second day increases, too. Hence the *independence* assumption is not fulfilled either.

*How could you (conceptually) improve the model with this in mind?*

We could take into account whether person tested has symptoms and whether there were previous contacts with positively tested colleagues. Based on this we would change the prior for employees who have higher risk of having COVID by shifting their prior to the right side. Since the prior has relatively low effect we would need to change it drastically. 




\newpage

# Exercise 3

We write the following function to use in this exercise:

```{r, echo = TRUE}
simulate_reg <- function(n, k = 1, alpha = 0, beta = 1, sigma = 1)  {
  
  # Remind the user to specify a vector as beta if they want to
  if ((length(beta) == 1)*(k>1) == 1) {
    warning("You have specified k > 1 but only one value for beta. 
              This function then uses the same beta for all independent variables!")
  }
  
  # Create an empty matrix for the independent variables
  X <- matrix(nrow = n, ncol = k)
  
  # Fill them with draws from normal distributions. For fun we let the
  # mean increase by 1 for every additional independent variable
  for(i in 1:k){
    X[,i] <- rnorm(n, i, 10)
  }
  
  # Make beta a vector of length k if it is only a scalar
  if ((length(beta) == 1) == TRUE) {
    beta <- rep(beta, k)
  }
  
  # Compute y = a + Xb + e, e ~ N(0,sigma)
  simul_dep <- rep(alpha, n) + X %*% beta + rnorm(n, 0, sigma)
  
  # Create one output data frame
  output_data <- data.frame(Y = simul_dep, X)
  
  # Return it
  return(output_data)
}
```



## Exercise 3.1 - Simulating Data with $k = 1$ and $\sigma = 1$

```{r, out.width="70%", fig.align="center", echo=F}
set.seed(2345)

sim_data <- simulate_reg(100, 1, 0, 1, 1)

plot(y = sim_data$Y, x = sim_data$X, main = "Simulated Data")

abline(lm(sim_data$Y ~ sim_data $ X), col = "red")

```



```{r, out.width="70%", fig.align="center", echo = F}
set.seed(2345)

coeffs <- vector(length = 1000)

for (i in 1:1000) {
  sim_data_2 <- simulate_reg(100, 1, 0, 1, 1)
  
  this_coefficient <- coef(lm(sim_data_2$Y ~ sim_data_2$X))[[2]]
  
  coeffs[i] <- this_coefficient
}

hist(coeffs, breaks = 100)

abline(v=1, col = "red")

legend(1.012,25, "True beta",
       lwd=2, col="red")

text(1.025,17, paste0("Mean of coeffs:\n",round(mean(coeffs),8)), col="blue")

```

The true value $\beta_0=1$. We see that if we run this regression over and over and over again, we get a distribution of estimated $\beta$s that is centered around the true value, but with some variance. Indeed, the mean of our estimated betas equals `r round(mean(coeffs),8)`, which is very close to $\beta_0=1$.

## Exercise 3.2 - Latent values if $\sigma$ is known

If $\sigma$ is known, $\alpha$ and $\beta$ are the only latent parameters.


## Exercise 3.3 - An interesting regression

Let's say we are interested in the determinants of our Econometrics grades, and we propose the model 

\[
  \mathrm{points}_i = \alpha + \mathrm{study\:time}\beta_1 + \mathrm{talent}\beta_2 + \mathrm{likeability}\beta_3 + u_i,
\]

i.e. we regress the total course points (0--100) of student $i$ on the time they devoted to studying, their inherent ability (assuming we can measure that), and how much they are deemed likable by the teachers of the course.

How can we come up with a prior distribution for $\beta_1$, the parameter associated with study time?

* It seems sensible to assume that the parameter associated with study time is **positive**, since it would be very weird if increasing the time devoted to studying for your Econometrics exam led to you getting a lower grade in your exam. If that were the case, nobody would be studying.

* We could even try to provide reasoning for a mean that we would want our prior distribution to be centered around. It would make sense if one hour of studying betters our result by, say, two points (and neither 0 nor 20). So we would use a prior distribution that is **centered at 2**.

* Then, we could assume for simplicity that the prior distribution is **normal**.

We could then go for one of the following priors:

```{r, echo = FALSE, out.width="70%", fig.align="center"}
plot(x = seq(-5, 9, length=1000), y = dnorm(seq(-5, 9, length=1000), 2, 1), type = "l", lwd = 2, col = "red", main = "Three Normal Prior Distributions with Mean 2", xlab = "", ylab = "")
lines(x = seq(-5, 9, length=1000), y = dnorm(seq(-5, 9, length=1000), 2, 2), type = "l", lwd = 2, col = "blue")
lines(x = seq(-5, 9, length=1000), y = dnorm(seq(-5, 9, length=1000), 2, 3), type = "l", lwd = 2, col = "orange")

legend(5,0.4, c("N(2,1)", "N(2,2)", "N(2,3)"),
       lwd=c(2,2,2), col=c("red", "blue", "orange"))
```

## Exercise 3.4 - Simulating data with known $\sigma$ and a Normal prior

We simulate data with $n \in\{50,100,200\}$, $k = 1$, $\alpha = 0$, $\beta = 1$ and $\sigma = 1$. We set the prior density to be $\mathcal{N}(0,1)$. Then, our posterior distribution will be given by

\[
  \mathcal{N}(\mu_n,\sigma_n^2),
\]

where 

\[
  \sigma_n = (\sigma_0^{-1}+\bm{X}'\bm{X})^{-1}
\]

and

\[
  \mu_n = \sigma_n(\sigma_o^{-1}\mu_0+\bm{X}'\bm{y}).
\]

This yields the following posterior distributions:

```{r, echo = FALSE, out.width="50%", fig.align = "center"}
set.seed(2345)

for (this_n in c(50, 100, 200)) {

this_data <- simulate_reg(this_n, 1, 0, 1, 1)

mu_0 <- 0
sigma_0 <- 1

sigma_n <- 1/ ((1/sigma_0) + t(this_data$X) %*% (this_data$X))

mu_n <- sigma_n * ((1/sigma_0) * mu_0 + t(this_data$X) %*% (this_data$Y))

plot(x = seq(0.98, 1.02, length=1000), y = dnorm(seq(0.98, 1.02, length=1000), mu_n, sigma_n), type = "l", lwd = 2, col = "red", main = paste0("n = ", this_n, ": Posterior ~ N(", round(mu_n,4), ", ", round(sigma_n,5),")"), xlab = "", ylab = "", yaxt = "n")

abline(v = 1, col = "blue", lwd = 2, lty = "dashed")
}
```

We can see that as we increase $n$, the priors' variance gets smaller and their mean moves closer to the true value.


\newpage

# Exercise 4
## Exercise 4.1 - Estimating $\mu$

For the given normally distributed data, the probability density of each observation $\small y_i$ looks like this: $\small p(y_i|\mu, 1)=(2\pi)^{-\frac{1}{2}}exp(-\frac{1}{2}(y_i-\mu)^2)$. 
For iid data, we furthermore have the following likelihood: $\small p(\boldsymbol{y}|\mu, 1)=\prod_{i=1}^{n}p(y_i|\mu, 1)=(2\pi)^{-\frac{n}{2}}exp(-\frac{1}{2}\sum_{i=1}^n(y_i-\mu)^2)$.

The normal pdf of the Normal prior is: $\small p(\mu|\mu_0, \sigma^2_0)= (2\pi\sigma^2_0)^{-\frac{1}{2}}exp(-\frac{1}{2}\frac{(\mu-\mu_0)^2}{\sigma^2_0})$.

To get to our posterior, we utilize Bayes' theorem, dropping all constant factors in the process, as they dont change proportionality.

\begin{align*}
\small p(\mu|\boldsymbol{y},1) \small\propto\: & \small p(\boldsymbol{y}|\mu, 1)\times p(\mu|\mu_0, \sigma^2_0)) \\
\small \propto\: & \small exp(-\frac{1}{2}\sum_{i=1}^n(y_i-\mu)^2)\times exp(-\frac{1}{2}\frac{(\mu-\mu_0)^2}{\sigma^2_0}) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\mu)^2)+\frac{1}{\sigma^2_0}(\mu-\mu_0)^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n(\bar{y}-\mu)^2+\frac{1}{\sigma^2_0}(\mu-\mu_0)^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2-2n\bar{y}\mu+n\mu^2+\frac{1}{\sigma^2_0}\mu^2-\frac{2}{\sigma^2_0}\mu\mu_0+\frac{1}{\sigma^2_0}\mu_0^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2+(n+\frac{1}{\sigma^2_0})\mu^2-2(n\bar{y}+\frac{1}{\sigma^2_0}\mu_0)\mu))
\end{align*}

For simplicity, we defined $\small \bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$, and used $\small \sum_{i=1}^{n}(y_i-\mu)^2=\sum_{i=1}^{n}(y_i-\bar{y}+\bar{y}-\mu)^2=\sum_{i=1}^{n}(y_i-\bar{y})^2+n(\bar{y}-\mu)^2$.
We can now say $\small \mu_n=(n+\frac{1}{\sigma^2_0})^{-1}(n\bar{y}+\frac{1}{\sigma^2_0}\mu_0)$ and $\small \sigma^2_n=(n+\frac{1}{\sigma^2_0})^{-1}$. 

After further simplification, our posterior yields:

\begin{align*}
\small p(\mu|\boldsymbol{y},1) \small\propto\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2-\frac{1}{\sigma^2_n}\mu_n^2+\frac{1}{\sigma^2_n}(\mu^2-2\mu\mu_n+\mu_n^2))) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2-\frac{1}{\sigma^2_n}\mu_n^2+\frac{1}{\sigma^2_n}(\mu-\mu_n)^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2-\frac{1}{\sigma^2_n}\mu_n^2)) \times exp(-\frac{1}{2\sigma^2_n}(\mu-\mu_n)^2)) \\
\small \propto\: & \small exp(-\frac{1}{2\sigma^2_n}(\mu-\mu_n)^2))
\end{align*}

We now see that the posterior is proportional to a $\small N(\mu_n, \sigma^2_n)$. 
The following plot shows two histograms with different prior distributions - for Prior 1 we have $\small \mu\sim N(0, 1)$ and for Prior 2 we use $\small \mu\sim N(10, 4)$.

```{r, fig.width=10, fig.height=5, echo = F}
set.seed(2345)
par(mfrow=c(1,2))
hist(rnorm(10^5, mean=0, sd=1), ylim=c(0, 20000),
     main="Prior 1", xlab=expression(mu), ylab=NA)
hist(rnorm(10^5, mean=10, sd=2), ylim=c(0, 20000),
     main="Prior 2", xlab=expression(mu), ylab=NA)
```


## Exercise 4.2 - Estimating $\sigma^2$

For the given normally distributed data, the probability density of each observation $\small y_i$ looks like this: $\small p(y_i|5, \sigma^2)=(2\pi\sigma^2)^{-\frac{1}{2}}exp(-\frac{1}{2}\frac{(y_i-5)^2}{\sigma^2})$. For iid data, the likelihood is given by:

\begin{align*}
\small p(\boldsymbol{y}|5, \sigma^2) \small=\: & \small \prod_{i=1}^{n}p(y_i|5, \sigma^2)=(2\pi\sigma^2)^{-\frac{n}{2}}exp(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-5)^2) \\
\small \propto\: & \small (\sigma^2)^{-\frac{n}{2}+1-1}exp(-\frac{\sum_{i=1}^n(y_i-5)^2/2}{\sigma^2}) \\
\small =\: & \small (\sigma^2)^{c-1}exp(-\frac{d}{\sigma^2})
\end{align*}

This expression is proportional to an Inverse-Gamma distribution $\small G^{-1}(c,d)$, with $\small c=-\frac{n}{2}+1$ and $\small d=\sum_{i=1}^n(y_i-5)^2/2$.

For the prior, we assume that the precision follows a $\small \sigma^{-2} \sim G(0.5, \eta)$. Therefore, we have $\small \sigma^2 \sim G^{-1}(0.5, \eta)$.^[See derivation of Inverse-Gamma from Gamma on Wikipedia. For the Gamma distribution $\small\beta$ denotes the *rate* parameter, whereas it denotes the *scale* parameter for the Inverse Gamma distribution!] 
This gives us the prior for $\small\sigma^2$ in the form of $\small p(\sigma^2|0.5, \eta)=\frac{\sqrt{\eta}}{\Gamma(0.5)}(\sigma^2)^{-1.5}exp(-\frac{\eta}{\sigma^2})$.

To get to our posterior, we again utilize Bayes' theorem and dropping constant terms:

\begin{align*}
\small p(\sigma^2|\boldsymbol{y},5) \small\propto\: & \small p(\boldsymbol{y}|5, \sigma^2)\times p(\sigma^2|0.5, \eta)) \\
\small \propto\: & \small (\sigma^2)^{-\frac{n}{2}}exp(-\frac{\sum_{i=1}^n(y_i-5)^2/2}{\sigma^2}) \times  (\sigma^2)^{-1.5}exp(-\frac{\eta}{\sigma^2}) \\
\small =\: & \small (\sigma^2)^{-(\frac{n}{2}+0.5)-1}exp(-\frac{1}{\sigma^2}(\eta+\sum_{i=1}^n(y_i-5)^2/2)) \\
\small =\: & \small (\sigma^2)^{-c_n-1}exp(-\frac{d_n}{\sigma^2})
\end{align*}

The posterior is thus proportional to an Inverse-Gamma distribution $\small G^{-1}(c_n, d_n)$, with $\small c_n=\frac{n}{2}+0.5$ and $\small d_n=\eta+\sum_{i=1}^n(y_i-5)^2/2$. The following plot visualizes the prior density for the wanted values of $\small \eta \in \{0.01, 1, 100\}$. On the left, we show the plot with respect to the precision $\small\sigma^{-2}$ and on the right with respect to variance $\small\sigma^2$.  


```{r, fig.width=10, fig.height=5, echo=F}
cols <- c("red", "darkblue", "purple")
eta <- c(0.01, 1, 100)
par(mfrow=c(1,2))
plot(NA,NA, xlim=c(0,10), ylim=c(0,1),
     main="Prior precision", xlab="precision", ylab=NA)
for (i in 1:length(eta)) {
  curve(dgamma(x, shape=.5, rate=eta[i]), from=0, to=10, col=cols[i], add=T, lwd=2)
}
plot(NA,NA, xlim=c(0,10), ylim=c(0,.5),
     main="Prior variance", xlab="variance", ylab=NA)
for (i in 1:length(eta)) {
  curve(invgamma::dinvgamma(x, shape=.5, scale=eta[i]), from=0, to=10, col=cols[i], add=T, lwd=2)
}
```

## Exercise 4.3 - A prior for $\eta$

A crucial consideration in prior choice it the one of allowed values - if we want to specify a prior for eg a variance, it makes no sense to use a Normal prior which allows for negative outcomes. A $\small\eta$ is the scale parameter of an Inverse-Gamma distribution, it has to be positive, but there is no upper bound. This criteria is fulfilled by several distributions, such as the Gamma, the Inverse-Gamma or a Log-Normal. As it does seem plausible from our data, we argue that $\small\sigma^2$ could be around 1, but we dont want to fully exclude larger values either; this could be achieved by drawing from a Inverse-Gamma with $\small\alpha=0.5$ and a scale parameter $\small\eta$ between 0.5 and 1. To get those values for $\small\eta$ often, we could draw $\small\eta$ from a $Lognormal(-0.4, 0.25)$. The plot below shows the density of this Log-Normal on the left and the actual prior we get for $\small\sigma^2$ on the right:

```{r, echo = F}
set.seed(2345)
library(dplyr)

eta_prior <- data.frame(eta=rlnorm(1000, meanlog=-.4, sdlog=.25), sigma2=NA)
for (i in 1:nrow(eta_prior)) {
  eta_prior$sigma2[i] <- invgamma::rinvgamma(1, shape=.5, scale=eta_prior$eta[i])
}

#For the plot
nice_etas <- eta_prior%>%filter(sigma2 <=10)%>%select(sigma2)


par(mfrow=c(1,2))
plot(NA,NA, xlim=c(0,2), ylim=c(0,2.5),
     main="The Lognormal density", xlab="Prior value", ylab=NA)
curve(dlnorm(x, meanlog=-.4, sdlog=.25), from=0, to=2, add=T, lwd=2)
plot(nice_etas,
     main=expression(paste("The " , sigma^2 ," Prior ")), xlab="Prior value", ylab=NA)
```

 
