---
title: '**Advanced Macroeconometrics -- Assignment 2**'
author:
  - "Max Heinze (h11742049@s.wu.ac.at)"
  - "Gabriel Konecny (h11775903@s.wu.ac.at)"
  - "Patrick Schüssele (h11712262@s.wu.ac.at)"
date: "May 10, 2023"
output: 
  pdf_document:
    toc: true
    includes:
      in_header: !expr file.path(rprojroot::find_rstudio_root_file(), "helper", "wrap_code.tex")
header-includes: 
   - \usepackage{tcolorbox}
   - \usepackage{bm}
papersize: a4
geometry: margin = 2cm
urlcolor: Mahogany
---

```{r, setup, include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
```

\vspace{2em}

\begin{tcolorbox}
\centering \itshape The executable code that was used in compiling the assignment is available on GitHub at \url{https://github.com/maxmheinze/macrometrics}.
\end{tcolorbox}

\newpage

# Exercise 1

## Convergence of Means of $n$ Normal Draws

We draw 100 times from a $\mathcal{N}(5,9)$ distribution and plot the cumulative mean as well as the expected value.

```{r, out.width="70%", fig.align="center"}
set.seed(2345)

n <- 100

normal_data <- data.frame(n=1:n, x=rnorm(n, mean=5, sd=9), mean=NA)

for (i in 1:n) {
  normal_data$mean[i] <- mean(normal_data$x[1:i])
}

plot(x=normal_data$n, y=normal_data$mean, type="l", lwd=2,
     main="Convergence of Means: Normal Distribution", xlab="n", ylab="")

abline(h=5, col="red", lwd=2)

legend(50,-2, c("Cumulative mean", "True mean"),
       lwd=c(2,2), col=c("black", "red"))
```

We can easily see that the cumulative mean, that is,

\[
  \frac{1}{n}\sum^n_{i=1}x_i, \quad n = 1,\dots,100,
\]

converges to the true mean, the expected value of the distribution, $\mathrm{E}(N(5,9))=5$.


## Convergence of Means of $n$ Draws From a Cauchy Distribution

Next, we draw 10,000 times from a Cauchy distribution as specified and plot the cumulative mean.

```{r, out.width="70%", fig.align="center"}
set.seed(2345)

n <- 10000

cauchy_data <- data.frame(n=1:n, x=rnorm(n, mean=0, sd=1)/rnorm(n, mean=0, sd=1), mean=NA)

for (i in 1:n) {
  cauchy_data$mean[i] <- mean(cauchy_data$x[1:i])
}

plot(x=cauchy_data$n, y=cauchy_data$mean, type="l", lwd=2,
     main="Convergence of Means: Cauchy Distribution", xlab="N draws", ylab="")

legend(5000, 5, "Cumulative mean", lwd=2, col="black")
```

This time, we do not observe convergence of the cumulative mean. Rather, we observe repeated random jumps in the cumulative mean. This is due to the Cauchy distribution not having an expected value that it could converge to.

\newpage

# Exercise 2 (STILL TO DO)




\newpage

# Exercise 3

## Writing a Function to Simulate $\bm{y}=\alpha+\bm{X}\bm{\beta}+\bm{e}$

```{r, echo = TRUE}
simulate_reg <- function(n, k = 1, alpha = 0, beta = 1, sigma = 1)  {
  
  # Remind the user to specify a vector as beta if they want to
  if ((length(beta) == 1)*(k>1) == 1) {
    warning("You have specified k > 1 but only one value for beta. 
              This function then uses the same beta for all independent variables!")
  }
  
  # Create an empty matrix for the independent variables
  X <- matrix(nrow = n, ncol = k)
  
  # Fill them with draws from normal distributions. For fun we let the
  # mean increase by 1 for every additional independent variable
  for(i in 1:k){
    X[,i] <- rnorm(n, i, 10)
  }
  
  # Make beta a vector of length k if it is only a scalar
  if ((length(beta) == 1) == TRUE) {
    beta <- rep(beta, k)
  }
  
  # Compute y = a + Xb + e, e ~ N(0,sigma)
  simul_dep <- rep(alpha, n) + X %*% beta + rnorm(n, 0, sigma)
  
  # Create one output data frame
  output_data <- data.frame(Y = simul_dep, X)
  
  # Return it
  return(output_data)
}
```


## Simulating Data with $k = 1$ and $\sigma = 1$

```{r, out.width="70%", fig.align="center"}
set.seed(2345)

sim_data <- simulate_reg(100, 1, 0, 1, 1)

plot(y = sim_data$Y, x = sim_data$X, main = "Simulated Data")

abline(lm(sim_data$Y ~ sim_data $ X), col = "red")

```



```{r, out.width="70%", fig.align="center"}
set.seed(2345)

coeffs <- vector(length = 1000)

for (i in 1:1000) {
  sim_data_2 <- simulate_reg(100, 1, 0, 1, 1)
  
  this_coefficient <- coef(lm(sim_data_2$Y ~ sim_data_2$X))[[2]]
  
  coeffs[i] <- this_coefficient
}

hist(coeffs, breaks = 100)

abline(v=1, col = "red")

legend(1.012,25, "True beta",
       lwd=2, col="red")

text(1.025,17, paste0("Mean of coeffs:\n",round(mean(coeffs),8)), col="blue")

```

The true value $\beta_0=1$. We see that if we run this regression over and over and over again, we get a distribution of estimated $\beta$s that is centered around the true value, but with some variance. Indeed, the mean of our estimated betas equals `r round(mean(coeffs),8)`, which is very close to $\beta_0=1$.

## Latent Values of the Model if we know $\sigma$

Let us say that we know $\sigma$. Then, since $\sigma$ is known, $\beta$ is the only latent parameter.

## Potentially Interesting Regression

Let's say we are interested in the determinants of our Econometrics grades, and we propose the model 

\[
  \mathrm{points}_i = \alpha + \mathrm{study\:time}\beta_1 + \mathrm{talent}\beta_2 + \mathrm{likeability}\beta_3 + u_i,
\]

i.e. we regress the total course points (0--100) of student $i$ on the time they devoted to studying, their inherent ability (assuming we can measure that), and how much they are deemed likable by the teachers of the course.

How can we come up with a prior distribution for $\beta_1$, the parameter associated with study time?

* It seems sensible to assume that the parameter associated with study time is **positive**, since it would be very weird if increasing the time devoted to studying for your Econometrics exam led to you getting a lower grade in your exam. If that were the case, nobody would be studying.

* We could even try to provide reasoning for a mean that we would want our prior distribution to be centered around. It would make sense if one hour of studying betters our result by, say, two points (and neither 0 nor 20). So we would use a prior distribution that is **centered at 2**.

* Then, we could assume for simplicity that the prior distribution is **normal**.

We could then go for one of the following priors:

```{r, out.width="70%", fig.align="center"}
plot(x = seq(-5, 9, length=1000), y = dnorm(seq(-5, 9, length=1000), 2, 1), type = "l", lwd = 2, col = "red", main = "Three Normal Prior Distributions with Mean 2", xlab = "", ylab = "")
lines(x = seq(-5, 9, length=1000), y = dnorm(seq(-5, 9, length=1000), 2, 2), type = "l", lwd = 2, col = "blue")
lines(x = seq(-5, 9, length=1000), y = dnorm(seq(-5, 9, length=1000), 2, 3), type = "l", lwd = 2, col = "orange")

legend(5,0.4, c("N(2,1)", "N(2,2)", "N(2,3)"),
       lwd=c(2,2,2), col=c("red", "blue", "orange"))
```

## Posterior Density for Simulated Data (STILL TO DO)

```{r}
simulate_reg(100, 1, 0, 1, 1)



```


\newpage

# Exercise 4 (NEEDS REWRITING)

## Estimating $\mu$

**Q:** Suppose you have data $\small \boldsymbol{y}\sim N(\mu, 1)$, and want to estimate $\small\mu$. Specify a Normal prior $\small \mu\sim N(\mu_0, \sigma^2_0)$.
Derive the posterior $\small p(\mu|\boldsymbol{y})$ by applying Bayes’ theorem. Create histograms of two priors of your choice.

First, as we assume our data $\small \boldsymbol{y}\sim N(\mu, 1)$, the probability density of a single observation $\small y_i$ is given by $\small p(y_i|\mu, 1)=(2\pi)^{-\frac{1}{2}}exp(-\frac{1}{2}(y_i-\mu)^2)$. If we assume our data to be i.i.d., then the joint density (the likelihood function) is given by $\small p(\boldsymbol{y}|\mu, 1)=\prod_{i=1}^{n}p(y_i|\mu, 1)=(2\pi)^{-\frac{n}{2}}exp(-\frac{1}{2}\sum_{i=1}^n(y_i-\mu)^2)$.

We further assume a normal prior $\small \mu\sim N(\mu_0, \sigma^2_0)$, hence $\small p(\mu|\mu_0, \sigma^2_0)= (2\pi\sigma^2_0)^{-\frac{1}{2}}exp(-\frac{1}{2}\frac{(\mu-\mu_0)^2}{\sigma^2_0})$.

To get to our posterior, we utilize Bayes' theorem, dropping all constant factors from our likelihood function and our prior in the process, since we are interested in proportionality:

\begin{align*}
\small p(\mu|\boldsymbol{y},1) \small\propto\: & \small p(\boldsymbol{y}|\mu, 1)\times p(\mu|\mu_0, \sigma^2_0)) \\
\small \propto\: & \small exp(-\frac{1}{2}\sum_{i=1}^n(y_i-\mu)^2)\times exp(-\frac{1}{2}\frac{(\mu-\mu_0)^2}{\sigma^2_0}) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\mu)^2)+\frac{1}{\sigma^2_0}(\mu-\mu_0)^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n(\bar{y}-\mu)^2+\frac{1}{\sigma^2_0}(\mu-\mu_0)^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2-2n\bar{y}\mu+n\mu^2+\frac{1}{\sigma^2_0}\mu^2-\frac{2}{\sigma^2_0}\mu\mu_0+\frac{1}{\sigma^2_0}\mu_0^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2+(n+\frac{1}{\sigma^2_0})\mu^2-2(n\bar{y}+\frac{1}{\sigma^2_0}\mu_0)\mu))
\end{align*}

Note that above, we define $\small \bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$, and utilized the fact that $\small \sum_{i=1}^{n}(y_i-\mu)^2=\sum_{i=1}^{n}(y_i-\bar{y}+\bar{y}-\mu)^2=\sum_{i=1}^{n}(y_i-\bar{y})^2+n(\bar{y}-\mu)^2$.
Now let $\small \mu_n=(n+\frac{1}{\sigma^2_0})^{-1}(n\bar{y}+\frac{1}{\sigma^2_0}\mu_0)$ and $\small \sigma^2_n=(n+\frac{1}{\sigma^2_0})^{-1}$. Then we can further simplify the posterior to:

\begin{align*}
\small p(\mu|\boldsymbol{y},1) \small\propto\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2-\frac{1}{\sigma^2_n}\mu_n^2+\frac{1}{\sigma^2_n}(\mu^2-2\mu\mu_n+\mu_n^2))) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2-\frac{1}{\sigma^2_n}\mu_n^2+\frac{1}{\sigma^2_n}(\mu-\mu_n)^2)) \\
\small =\: & \small exp(-\frac{1}{2}(\sum_{i=1}^n(y_i-\bar{y})^2)+n\bar{y}^2+\frac{1}{\sigma^2_0}\mu_0^2-\frac{1}{\sigma^2_n}\mu_n^2)) \times exp(-\frac{1}{2\sigma^2_n}(\mu-\mu_n)^2)) \\
\small \propto\: & \small exp(-\frac{1}{2\sigma^2_n}(\mu-\mu_n)^2))
\end{align*}

Hence we find that the posterior is proportional to the density of a distribution $\small N(\mu_n, \sigma^2_n)$. In the plot below, we further visualized the histograms of two prior distributions, where in Prior 1 we have $\small \mu\sim N(0, 1)$ and for Prior 2 we specified $\small \mu\sim N(100, 25)$.

```{r, fig.width=10, fig.height=5}
set.seed(123)
par(mfrow=c(1,2))
hist(rnorm(100000, mean=0, sd=1), ylim=c(0, 20000),
     main="Prior 1", xlab=expression(mu), ylab=NA)
hist(rnorm(100000, mean=100, sd=5), ylim=c(0, 20000),
     main="Prior 2", xlab=expression(mu), ylab=NA)
```


## Estimating $\sigma^2$

**Q:** Suppose you have data $\small \boldsymbol{y}\sim N(5, \sigma^2)$, and want to estimate $\small\sigma^2$. Work with the precision, $\small\sigma^{-2}$, and specify a Gamma prior $\small \sigma^{-2} \sim G(0.5, \eta)$ with single parameter $\small\eta$. Derive the posterior $p(\sigma^2|\boldsymbol{y})$ by applying Bayes’ theorem. Visualise the prior density for $\small \eta \in \{0.01, 1, 100\}$.

First, as we assume our data $\small \boldsymbol{y}\sim N(5, \sigma^2)$, the probability density of a single observation $\small y_i$ is given by $\small p(y_i|5, \sigma^2)=(2\pi\sigma^2)^{-\frac{1}{2}}exp(-\frac{1}{2}\frac{(y_i-5)^2}{\sigma^2})$. If we assume our data to be i.i.d., then the joint density (the likelihood function) is given by:

\begin{align*}
\small p(\boldsymbol{y}|5, \sigma^2) \small=\: & \small \prod_{i=1}^{n}p(y_i|5, \sigma^2)=(2\pi\sigma^2)^{-\frac{n}{2}}exp(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-5)^2) \\
\small \propto\: & \small (\sigma^2)^{-\frac{n}{2}+1-1}exp(-\frac{\sum_{i=1}^n(y_i-5)^2/2}{\sigma^2}) \\
\small =\: & \small (\sigma^2)^{c-1}exp(-\frac{d}{\sigma^2})
\end{align*}

Hence, we find that the likelihood is proportional to an Inverse-Gamma distribution $\small G^{-1}(c,d)$, where $\small c=-\frac{n}{2}+1$ and $\small d=\sum_{i=1}^n(y_i-5)^2/2$.

For the prior we assume that the precision follows a Gamma distribution, i.e. $\small \sigma^{-2} \sim G(0.5, \eta)$. Therefore, we have $\small \sigma^2 \sim G^{-1}(0.5, \eta)$.^[See derivation of Inverse-Gamma from Gamma on  [Wikipedia](https://en.wikipedia.org/wiki/Inverse-gamma_distribution#Derivation_from_Gamma_distribution). Note that whereas for the Gamma distribution $\small\beta$ is the rate parameter, it is the scale parameter for the Inverse Gamma distribution.] Our prior for $\small\sigma^2$ is then given by $\small p(\sigma^2|0.5, \eta)=\frac{\sqrt{\eta}}{\Gamma(0.5)}(\sigma^2)^{-1.5}exp(-\frac{\eta}{\sigma^2})$.

To get to our posterior, we utilize Bayes' theorem, dropping all constant factors from our likelihood function and our prior in the process, since we are interested in proportionality:

\begin{align*}
\small p(\sigma^2|\boldsymbol{y},5) \small\propto\: & \small p(\boldsymbol{y}|5, \sigma^2)\times p(\sigma^2|0.5, \eta)) \\
\small \propto\: & \small (\sigma^2)^{-\frac{n}{2}}exp(-\frac{\sum_{i=1}^n(y_i-5)^2/2}{\sigma^2}) \times  (\sigma^2)^{-1.5}exp(-\frac{\eta}{\sigma^2}) \\
\small =\: & \small (\sigma^2)^{-(\frac{n}{2}+0.5)-1}exp(-\frac{1}{\sigma^2}(\eta+\sum_{i=1}^n(y_i-5)^2/2)) \\
\small =\: & \small (\sigma^2)^{-c_n-1}exp(-\frac{d_n}{\sigma^2})
\end{align*}

Therefore, the posterior is proportional to the pdf of an Inverse-Gamma distribution $\small G^{-1}(c_n, d_n)$, where $\small c_n=\frac{n}{2}+0.5$ and $\small d_n=\eta+\sum_{i=1}^n(y_i-5)^2/2$. In the plot below, we visualised the prior density for $\small \eta \in \{0.01, 1, 100\}$ for both the precision $\small\sigma^{-2}$ (on the left) as well as the variance $\small\sigma^2$ (on the right).


```{r, fig.width=10, fig.height=5}
cols <- c("red", "blue", "orange")
eta <- c(0.01, 1, 100)
par(mfrow=c(1,2))
plot(NA,NA, xlim=c(0,10), ylim=c(0,1),
     main="Prior: Precision", xlab=expression(1/sigma^2), ylab=NA)
for (i in 1:length(eta)) {
  curve(dgamma(x, shape=.5, rate=eta[i]), from=0, to=10, col=cols[i], add=T, lwd=2)
}
plot(NA,NA, xlim=c(0,10), ylim=c(0,.5),
     main="Prior: Variance", xlab=expression(sigma^2), ylab=NA)
for (i in 1:length(eta)) {
  curve(invgamma::dinvgamma(x, shape=.5, scale=eta[i]), from=0, to=10, col=cols[i], add=T, lwd=2)
}
legend(3,.9, c("eta1=0.01", "eta2=1", "eta3=100"),
       lwd=2, col=cols)
```

## Including $\eta$

First, the hyperprior should be chosen such that the support of this distribution fits the values that $\small\eta$ is allowed to take. As the scale parameter of an Inverse-Gamma distribution has to be larger than 0, we need to find a hyperprior such that random draws result in $\small\eta\in(0,\infty)$. Several distributions such as the Gamma, Inverse-Gamma, or the Log-Normal distribution fullfill this criteria. Now from the type of data that we are working with, we may have some kind of an idea as to where $\small\sigma^2$ should be. Let's say that we are quite sure that $\small\sigma^2$ is most likely to be around 1, but we also want to allow for larger error terms. Then this would correspond to the density of an Inverse-Gamma function with $\small\alpha=0.5$ (which is fixed by assumption) and a scale parameter $\small\eta$ that is most of the time somewhere between 0.5 and 1. This can be achieved by drawing from $\small\eta\sim Lognormal(-0.4, 0.25)$ (see the density for this distribution on the left hand side of the plot below). The prior $\small\sigma^2|\eta$ is visualized on the right.

```{r}
set.seed(123)
eta.prior <- data.frame(eta=rlnorm(1000, meanlog=-.4, sdlog=.25), sigma2=NA)
for (i in 1:nrow(eta.prior)) {
  eta.prior$sigma2[i] <- invgamma::rinvgamma(1, shape=.5, scale=eta.prior$eta[i])
}
par(mfrow=c(1,2))
plot(NA,NA, xlim=c(0,2), ylim=c(0,2.5),
     main="Density: Lognormal(-0.4, 0.25)", xlab=expression(eta), ylab=NA)
curve(dlnorm(x, meanlog=-.4, sdlog=.25), from=0, to=2, add=T, lwd=2)
plot(density(eta.prior$sigma2),
     main=expression(paste("Prior: ", sigma^2, "|", eta)), xlab=expression(sigma^2), ylab=NA)
```

 